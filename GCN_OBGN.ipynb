{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/M4mbo/GCN_for_Node_Prediction_of_OGBN/blob/main/GCN_OBGN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OGBN-Products"
      ],
      "metadata": {
        "id": "g_Vh9kosaChl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ogbn-products dataset is an undirected and unweighted graph, representing an Amazon product co-purchasing network. Nodes represent products sold in Amazon, and edges between two products indicate that the products are purchased together. Node features are generated by extracting bag-of-words features from the product descriptions followed by a Principal Component Analysis to reduce the dimension to 100.\n",
        "\n",
        "The task is to predict the category of a product in a multi-class classification setup, where the 47 top-level categories are used for target labels."
      ],
      "metadata": {
        "id": "8HFHJC-LaLyi"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vkP8pA1qBE5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "443719d5-f616-475f-89f3-4c4c9024ad3e"
      },
      "source": [
        "import torch\n",
        "import os\n",
        "print(\"PyTorch has version {}\".format(torch.__version__))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch has version 2.0.1+cu118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zr8hfxJ-qRg2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f758c44-62dd-43c3-8e8a-7f45d2f4de0b"
      },
      "source": [
        "# Install torch geometric\n",
        "!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-{torch.__version__}.html\n",
        "!pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-{torch.__version__}.html\n",
        "!pip install torch-geometric\n",
        "!pip install ogb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.0.1+cu118.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_scatter-2.1.1%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (10.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.1+pt20cu118\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.0.1+cu118.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_sparse-0.6.17%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (4.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.10.1)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.22.4)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.17+pt20cu118\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.3.1.tar.gz (661 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.6/661.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.27.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.2.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.3.1-py3-none-any.whl size=910460 sha256=cabab0a4f1931f5de4b457a0b971187c1da94e8f228dfb611c484da31118fcef\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/dc/30/e2874821ff308ee67dcd7a66dbde912411e19e35a1addda028\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.3.1\n",
            "Collecting ogb\n",
            "  Downloading ogb-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.0.1+cu118)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.22.4)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (4.65.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.2.2)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.5.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.26.16)\n",
            "Collecting outdated>=0.2.0 (from ogb)\n",
            "  Downloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (67.7.2)\n",
            "Collecting littleutils (from outdated>=0.2.0->ogb)\n",
            "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (2.27.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2022.7.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.3.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->ogb) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->ogb) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->ogb) (2.1.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->ogb) (1.3.0)\n",
            "Building wheels for collected packages: littleutils\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7028 sha256=e1bd01f1534cccb72f90642b61265d0ec7de520cd96b0bf02e14530bc957090d\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/fe/b0/27a9892da57472e538c7452a721a9cf463cc03cf7379889266\n",
            "Successfully built littleutils\n",
            "Installing collected packages: littleutils, outdated, ogb\n",
            "Successfully installed littleutils-0.2.2 ogb-1.3.6 outdated-0.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.data import DataLoader\n",
        "import numpy as np\n",
        "from torch_geometric.typing import SparseTensor\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "3SkS1Mzcbe8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IK9z0wQIwzQ"
      },
      "source": [
        "## Load and Preprocess the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ibJ0ieoIwQM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01e5b6a4-3f1d-4b0f-a663-60377c31011f"
      },
      "source": [
        "dataset_name = 'ogbn-products'\n",
        "dataset = PygNodePropPredDataset(name=dataset_name,\n",
        "                                 transform=T.ToSparseTensor())\n",
        "data = dataset[0]\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# If you use GPU, the device should be cuda\n",
        "print('Device: {}'.format(device))\n",
        "\n",
        "# data = data.to(device)\n",
        "# split_idx = dataset.get_idx_split()\n",
        "# train_idx = split_idx['train'].to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This will download 1.38GB. Will you proceed? (y/N)\n",
            "y\n",
            "Downloading http://snap.stanford.edu/ogb/data/nodeproppred/products.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloaded 1.38 GB: 100%|██████████| 1414/1414 [00:39<00:00, 35.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset/products.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading necessary files...\n",
            "This might take a while.\n",
            "Processing graphs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:01<00:00,  1.79s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting graphs into PyG objects...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 95.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check task type\n",
        "print('Task type: {}'.format(dataset.task_type))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lp8pnCy5-j7l",
        "outputId": "1f9116cb-08e5-4f3b-f4f3-21d282d967e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task type: multiclass classification\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJ6I9prjdg57",
        "outputId": "ed4488d0-f688-4f56-82a9-5e53188a586c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Data(num_nodes=2449029, x=[2449029, 100], y=[2449029, 1], adj_t=[2449029, 2449029, nnz=123718280])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.y.unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-AYG-jcLyIl",
        "outputId": "806a228d-cd69-4628-8440-bf6b41a2a6f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
              "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
              "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We need to have edge indxes to make a subgraph. We can get those from the adjacency matrix.\n",
        "data.edge_index = torch.stack([data.adj_t.__dict__[\"storage\"]._row, data.adj_t.__dict__[\"storage\"]._col])\n",
        "\n",
        "# We will only use the first 100000 nodes.\n",
        "sub_nodes = 100000\n",
        "sub_graph = data.subgraph(torch.arange(sub_nodes))\n",
        "\n",
        "# Update the adjaceny matrix according to the new graph\n",
        "sub_graph.adj_t = SparseTensor(\n",
        "    row=sub_graph.edge_index[0],\n",
        "    col=sub_graph.edge_index[1],\n",
        "    sparse_sizes=None,\n",
        "    is_sorted=True,\n",
        "    trust_data=True,\n",
        ")\n",
        "\n",
        "sub_graph = sub_graph.to(device)\n",
        "\n",
        "sub_graph\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jN80UY6bALzs",
        "outputId": "be336a98-3018-417b-e416-c96c6176f172"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Data(num_nodes=100000, x=[100000, 100], y=[100000, 1], adj_t=[100000, 100000, nnz=2818046], edge_index=[2, 2818046])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Spilt data into train validation and test set\n",
        "split_sizes = [int(sub_nodes*0.8),int(sub_nodes*0.05),int(sub_nodes*0.15)]\n",
        "indices = torch.arange(sub_nodes)\n",
        "np.random.shuffle(indices.numpy())\n",
        "split_idx = {s:t for t,s in zip(torch.split(indices, split_sizes, dim=0), [\"train\", \"valid\", \"test\"])}\n",
        "split_idx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6_6YFEBApJq",
        "outputId": "3f1b14b7-c6ef-403c-e210-50d2f599dd65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': tensor([92152, 15253, 11721,  ..., 47230, 83100, 70595]),\n",
              " 'valid': tensor([80764, 51728, 62769,  ..., 89128, 23407, 54534]),\n",
              " 'test': tensor([41972,  4090, 11810,  ..., 65555, 79150, 94116])}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set batch size for data loaders\n",
        "batch_size = 32  # You can change this to your preferred batch size\n",
        "\n",
        "# Split indices for train, validation, and test sets\n",
        "train_idx = split_idx[\"train\"]\n",
        "valid_idx = split_idx[\"valid\"]\n",
        "test_idx = split_idx[\"test\"]"
      ],
      "metadata": {
        "id": "q3Nrhnno6ylE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgUA815bNJ8w"
      },
      "source": [
        "## GCN Model\n",
        "\n",
        "Now we will implement our GCN model!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "args = {\n",
        "    'device': device,\n",
        "    'num_layers': 5,\n",
        "    'hidden_dim': 256,\n",
        "    'dropout': 0.5,\n",
        "    'lr': 0.001,\n",
        "    'epochs': 30,\n",
        "}\n",
        "args"
      ],
      "metadata": {
        "id": "bKuGGzv3_NOt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e62b1dbd-8bed-4a2c-d9a4-67feb5eda899"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'device': 'cuda',\n",
              " 'num_layers': 5,\n",
              " 'hidden_dim': 256,\n",
              " 'dropout': 0.5,\n",
              " 'lr': 0.001,\n",
              " 'epochs': 30}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers,\n",
        "                 dropout, return_embeds=False):\n",
        "\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        # A list of GCNConv layers\n",
        "        self.convs = [\n",
        "            GCNConv(input_dim, hidden_dim),\n",
        "        ]\n",
        "\n",
        "        for _ in range(num_layers-1):\n",
        "            self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
        "\n",
        "        self.last_layer = GCNConv(hidden_dim, output_dim)\n",
        "\n",
        "\n",
        "        # A list of 1D batch normalization layers\n",
        "        self.bns = []\n",
        "        for _ in range(num_layers):\n",
        "            self.bns.append(torch.nn.BatchNorm1d(hidden_dim))\n",
        "\n",
        "        self.convs = torch.nn.ModuleList(self.convs)\n",
        "        self.bns = torch.nn.ModuleList(self.bns)\n",
        "\n",
        "        # The log softmax layer\n",
        "        self.softmax = torch.nn.LogSoftmax()\n",
        "\n",
        "\n",
        "        # Probability of an element getting zeroed\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # Skip classification layer and return node embeddings\n",
        "        self.return_embeds = return_embeds\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "        for bn in self.bns:\n",
        "            bn.reset_parameters()\n",
        "\n",
        "    def forward(self, x, adj_t):\n",
        "\n",
        "        for i in range(len(self.convs)):\n",
        "            x = self.convs[i](x, adj_t)\n",
        "            x = self.bns[i](x)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        x = self.last_layer(x, adj_t)\n",
        "\n",
        "        if not self.return_embeds:\n",
        "          x = self.softmax(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "318M7d48P3JI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, data, train_idx, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    loss = 0\n",
        "\n",
        "    optimizer.zero_grad()  # Clear gradients.\n",
        "    out = model(data.x, data.adj_t)  # Perform a single forward pass.\n",
        "    loss = loss_fn(out[train_idx], sub_graph.y[train_idx].reshape(-1))\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()"
      ],
      "metadata": {
        "id": "kv8FysMPTpy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test function here\n",
        "@torch.no_grad()\n",
        "def test(model, data, split_idx, evaluator, save_model_results=False):\n",
        "    model.eval()\n",
        "\n",
        "    # The output of model on all data\n",
        "    out = model(data.x, data.adj_t)\n",
        "\n",
        "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
        "\n",
        "    train_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['train']],\n",
        "        'y_pred': y_pred[split_idx['train']],\n",
        "    })['acc']\n",
        "    valid_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['valid']],\n",
        "        'y_pred': y_pred[split_idx['valid']],\n",
        "    })['acc']\n",
        "    test_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['test']],\n",
        "        'y_pred': y_pred[split_idx['test']],\n",
        "    })['acc']\n",
        "\n",
        "    if save_model_results:\n",
        "      print (\"Saving Model Predictions\")\n",
        "\n",
        "      data = {}\n",
        "      data['y_pred'] = y_pred.view(-1).cpu().detach().numpy()\n",
        "\n",
        "      df = pd.DataFrame(data=data)\n",
        "      # Save locally as csv\n",
        "      df.to_csv('ogbn-arxiv_node.csv', sep=',', index=False)\n",
        "\n",
        "\n",
        "    return train_acc, valid_acc, test_acc"
      ],
      "metadata": {
        "id": "cTjUhE2wT4sV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = {\n",
        "    'device': device,\n",
        "    'num_layers': 3,\n",
        "    'hidden_dim': 256,\n",
        "    'dropout': 0.5,\n",
        "    'lr': 0.01,\n",
        "    'epochs': 200,\n",
        "}\n",
        "args"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPSbM-YcT_Ik",
        "outputId": "ff89bcc5-29a0-42b7-99ca-10445bb08253"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'device': 'cuda',\n",
              " 'num_layers': 3,\n",
              " 'hidden_dim': 256,\n",
              " 'dropout': 0.5,\n",
              " 'lr': 0.01,\n",
              " 'epochs': 200}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GCN(sub_graph.num_features, args['hidden_dim'],\n",
        "            dataset.num_classes, args['num_layers'],\n",
        "            args['dropout']).to(device)\n",
        "evaluator = Evaluator(name='ogbn-arxiv')"
      ],
      "metadata": {
        "id": "UUhV70xiUhKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "# reset the parameters to initial random value\n",
        "model.reset_parameters()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
        "loss_fn = F.nll_loss\n",
        "\n",
        "best_model = None\n",
        "best_valid_acc = 0\n",
        "\n",
        "for epoch in range(1, 1 + args[\"epochs\"]):\n",
        "  loss = train(model, sub_graph, train_idx, optimizer, loss_fn)\n",
        "  result = test(model, sub_graph, split_idx, evaluator)\n",
        "  train_acc, valid_acc, test_acc = result\n",
        "  if valid_acc > best_valid_acc:\n",
        "      best_valid_acc = valid_acc\n",
        "      best_model = copy.deepcopy(model)\n",
        "\n",
        "  print(f'Epoch: {epoch:02d}, '\n",
        "        f'Loss: {loss:.4f}, '\n",
        "        f'Train: {100 * train_acc:.2f}%, '\n",
        "        f'Valid: {100 * valid_acc:.2f}% '\n",
        "        f'Test: {100 * test_acc:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQblrAdGUkE9",
        "outputId": "a94ba9b1-2950-4d10-c56a-1d0afbbf52d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-bd2bcc944472>:53: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  x = self.softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01, Loss: 3.8963, Train: 50.43%, Valid: 50.06% Test: 50.41%\n",
            "Epoch: 02, Loss: 1.7607, Train: 72.49%, Valid: 72.56% Test: 72.75%\n",
            "Epoch: 03, Loss: 1.0830, Train: 77.19%, Valid: 77.14% Test: 77.43%\n",
            "Epoch: 04, Loss: 0.8898, Train: 79.53%, Valid: 78.76% Test: 79.73%\n",
            "Epoch: 05, Loss: 0.8202, Train: 81.72%, Valid: 81.16% Test: 81.95%\n",
            "Epoch: 06, Loss: 0.7581, Train: 82.81%, Valid: 82.40% Test: 82.83%\n",
            "Epoch: 07, Loss: 0.7150, Train: 83.43%, Valid: 83.06% Test: 83.46%\n",
            "Epoch: 08, Loss: 0.6817, Train: 83.97%, Valid: 83.42% Test: 84.01%\n",
            "Epoch: 09, Loss: 0.6487, Train: 84.72%, Valid: 84.02% Test: 84.73%\n",
            "Epoch: 10, Loss: 0.6279, Train: 85.33%, Valid: 84.78% Test: 85.38%\n",
            "Epoch: 11, Loss: 0.6131, Train: 85.99%, Valid: 85.50% Test: 86.23%\n",
            "Epoch: 12, Loss: 0.5952, Train: 86.50%, Valid: 85.88% Test: 86.73%\n",
            "Epoch: 13, Loss: 0.5829, Train: 86.68%, Valid: 86.22% Test: 86.81%\n",
            "Epoch: 14, Loss: 0.5727, Train: 86.86%, Valid: 86.38% Test: 87.01%\n",
            "Epoch: 15, Loss: 0.5639, Train: 87.00%, Valid: 86.68% Test: 87.24%\n",
            "Epoch: 16, Loss: 0.5532, Train: 87.18%, Valid: 86.74% Test: 87.37%\n",
            "Epoch: 17, Loss: 0.5414, Train: 87.30%, Valid: 86.78% Test: 87.51%\n",
            "Epoch: 18, Loss: 0.5346, Train: 87.38%, Valid: 86.90% Test: 87.39%\n",
            "Epoch: 19, Loss: 0.5251, Train: 87.44%, Valid: 86.84% Test: 87.48%\n",
            "Epoch: 20, Loss: 0.5187, Train: 87.52%, Valid: 87.02% Test: 87.55%\n",
            "Epoch: 21, Loss: 0.5126, Train: 87.59%, Valid: 87.10% Test: 87.66%\n",
            "Epoch: 22, Loss: 0.5078, Train: 87.69%, Valid: 87.10% Test: 87.86%\n",
            "Epoch: 23, Loss: 0.4979, Train: 87.82%, Valid: 87.26% Test: 87.99%\n",
            "Epoch: 24, Loss: 0.4942, Train: 87.90%, Valid: 87.34% Test: 88.02%\n",
            "Epoch: 25, Loss: 0.4897, Train: 87.98%, Valid: 87.44% Test: 88.07%\n",
            "Epoch: 26, Loss: 0.4818, Train: 88.06%, Valid: 87.44% Test: 88.22%\n",
            "Epoch: 27, Loss: 0.4771, Train: 88.12%, Valid: 87.34% Test: 88.14%\n",
            "Epoch: 28, Loss: 0.4729, Train: 88.16%, Valid: 87.36% Test: 88.24%\n",
            "Epoch: 29, Loss: 0.4683, Train: 88.20%, Valid: 87.38% Test: 88.25%\n",
            "Epoch: 30, Loss: 0.4643, Train: 88.24%, Valid: 87.54% Test: 88.27%\n",
            "Epoch: 31, Loss: 0.4601, Train: 88.33%, Valid: 87.72% Test: 88.30%\n",
            "Epoch: 32, Loss: 0.4592, Train: 88.45%, Valid: 87.82% Test: 88.34%\n",
            "Epoch: 33, Loss: 0.4541, Train: 88.59%, Valid: 87.92% Test: 88.59%\n",
            "Epoch: 34, Loss: 0.4504, Train: 88.69%, Valid: 87.98% Test: 88.75%\n",
            "Epoch: 35, Loss: 0.4454, Train: 88.72%, Valid: 87.92% Test: 88.72%\n",
            "Epoch: 36, Loss: 0.4469, Train: 88.78%, Valid: 87.98% Test: 88.71%\n",
            "Epoch: 37, Loss: 0.4413, Train: 88.83%, Valid: 88.02% Test: 88.71%\n",
            "Epoch: 38, Loss: 0.4391, Train: 88.88%, Valid: 88.04% Test: 88.87%\n",
            "Epoch: 39, Loss: 0.4319, Train: 88.91%, Valid: 88.08% Test: 88.73%\n",
            "Epoch: 40, Loss: 0.4303, Train: 88.94%, Valid: 88.18% Test: 88.74%\n",
            "Epoch: 41, Loss: 0.4272, Train: 89.10%, Valid: 88.18% Test: 88.88%\n",
            "Epoch: 42, Loss: 0.4248, Train: 89.18%, Valid: 88.10% Test: 88.93%\n",
            "Epoch: 43, Loss: 0.4236, Train: 89.28%, Valid: 88.26% Test: 89.02%\n",
            "Epoch: 44, Loss: 0.4220, Train: 89.31%, Valid: 88.30% Test: 89.05%\n",
            "Epoch: 45, Loss: 0.4194, Train: 89.40%, Valid: 88.36% Test: 89.09%\n",
            "Epoch: 46, Loss: 0.4155, Train: 89.43%, Valid: 88.40% Test: 89.11%\n",
            "Epoch: 47, Loss: 0.4138, Train: 89.44%, Valid: 88.34% Test: 89.19%\n",
            "Epoch: 48, Loss: 0.4111, Train: 89.45%, Valid: 88.36% Test: 89.21%\n",
            "Epoch: 49, Loss: 0.4083, Train: 89.48%, Valid: 88.40% Test: 89.21%\n",
            "Epoch: 50, Loss: 0.4089, Train: 89.58%, Valid: 88.46% Test: 89.31%\n",
            "Epoch: 51, Loss: 0.4072, Train: 89.62%, Valid: 88.44% Test: 89.32%\n",
            "Epoch: 52, Loss: 0.4053, Train: 89.64%, Valid: 88.50% Test: 89.38%\n",
            "Epoch: 53, Loss: 0.4036, Train: 89.74%, Valid: 88.38% Test: 89.45%\n",
            "Epoch: 54, Loss: 0.4025, Train: 89.76%, Valid: 88.44% Test: 89.50%\n",
            "Epoch: 55, Loss: 0.4015, Train: 89.80%, Valid: 88.56% Test: 89.47%\n",
            "Epoch: 56, Loss: 0.3972, Train: 89.83%, Valid: 88.52% Test: 89.48%\n",
            "Epoch: 57, Loss: 0.3964, Train: 89.90%, Valid: 88.66% Test: 89.57%\n",
            "Epoch: 58, Loss: 0.3926, Train: 89.91%, Valid: 88.60% Test: 89.59%\n",
            "Epoch: 59, Loss: 0.3935, Train: 89.92%, Valid: 88.50% Test: 89.58%\n",
            "Epoch: 60, Loss: 0.3928, Train: 89.98%, Valid: 88.62% Test: 89.61%\n",
            "Epoch: 61, Loss: 0.3891, Train: 90.00%, Valid: 88.58% Test: 89.67%\n",
            "Epoch: 62, Loss: 0.3877, Train: 90.03%, Valid: 88.70% Test: 89.79%\n",
            "Epoch: 63, Loss: 0.3850, Train: 90.04%, Valid: 88.76% Test: 89.78%\n",
            "Epoch: 64, Loss: 0.3858, Train: 90.08%, Valid: 88.84% Test: 89.73%\n",
            "Epoch: 65, Loss: 0.3847, Train: 90.10%, Valid: 88.76% Test: 89.78%\n",
            "Epoch: 66, Loss: 0.3812, Train: 90.13%, Valid: 88.90% Test: 89.73%\n",
            "Epoch: 67, Loss: 0.3806, Train: 90.18%, Valid: 88.96% Test: 89.87%\n",
            "Epoch: 68, Loss: 0.3812, Train: 90.19%, Valid: 89.12% Test: 89.95%\n",
            "Epoch: 69, Loss: 0.3791, Train: 90.19%, Valid: 89.00% Test: 89.83%\n",
            "Epoch: 70, Loss: 0.3779, Train: 90.26%, Valid: 88.96% Test: 89.94%\n",
            "Epoch: 71, Loss: 0.3769, Train: 90.25%, Valid: 88.94% Test: 89.93%\n",
            "Epoch: 72, Loss: 0.3759, Train: 90.32%, Valid: 89.16% Test: 89.95%\n",
            "Epoch: 73, Loss: 0.3728, Train: 90.25%, Valid: 89.16% Test: 89.76%\n",
            "Epoch: 74, Loss: 0.3726, Train: 90.30%, Valid: 89.24% Test: 89.82%\n",
            "Epoch: 75, Loss: 0.3701, Train: 90.29%, Valid: 89.06% Test: 89.88%\n",
            "Epoch: 76, Loss: 0.3710, Train: 90.37%, Valid: 89.16% Test: 89.89%\n",
            "Epoch: 77, Loss: 0.3697, Train: 90.34%, Valid: 89.18% Test: 89.83%\n",
            "Epoch: 78, Loss: 0.3676, Train: 90.39%, Valid: 89.20% Test: 89.96%\n",
            "Epoch: 79, Loss: 0.3655, Train: 90.34%, Valid: 89.12% Test: 89.94%\n",
            "Epoch: 80, Loss: 0.3658, Train: 90.49%, Valid: 89.24% Test: 90.09%\n",
            "Epoch: 81, Loss: 0.3629, Train: 90.50%, Valid: 89.20% Test: 90.13%\n",
            "Epoch: 82, Loss: 0.3645, Train: 90.48%, Valid: 89.22% Test: 90.05%\n",
            "Epoch: 83, Loss: 0.3636, Train: 90.54%, Valid: 89.28% Test: 90.10%\n",
            "Epoch: 84, Loss: 0.3610, Train: 90.56%, Valid: 89.42% Test: 90.05%\n",
            "Epoch: 85, Loss: 0.3578, Train: 90.58%, Valid: 89.36% Test: 90.10%\n",
            "Epoch: 86, Loss: 0.3591, Train: 90.58%, Valid: 89.38% Test: 90.14%\n",
            "Epoch: 87, Loss: 0.3589, Train: 90.60%, Valid: 89.34% Test: 90.16%\n",
            "Epoch: 88, Loss: 0.3562, Train: 90.64%, Valid: 89.18% Test: 90.11%\n",
            "Epoch: 89, Loss: 0.3569, Train: 90.67%, Valid: 89.52% Test: 90.22%\n",
            "Epoch: 90, Loss: 0.3575, Train: 90.69%, Valid: 89.50% Test: 90.21%\n",
            "Epoch: 91, Loss: 0.3560, Train: 90.76%, Valid: 89.40% Test: 90.17%\n",
            "Epoch: 92, Loss: 0.3520, Train: 90.76%, Valid: 89.36% Test: 90.29%\n",
            "Epoch: 93, Loss: 0.3527, Train: 90.75%, Valid: 89.42% Test: 90.30%\n",
            "Epoch: 94, Loss: 0.3521, Train: 90.83%, Valid: 89.56% Test: 90.38%\n",
            "Epoch: 95, Loss: 0.3503, Train: 90.82%, Valid: 89.48% Test: 90.37%\n",
            "Epoch: 96, Loss: 0.3491, Train: 90.87%, Valid: 89.56% Test: 90.41%\n",
            "Epoch: 97, Loss: 0.3498, Train: 90.83%, Valid: 89.54% Test: 90.38%\n",
            "Epoch: 98, Loss: 0.3468, Train: 90.87%, Valid: 89.70% Test: 90.32%\n",
            "Epoch: 99, Loss: 0.3460, Train: 90.89%, Valid: 89.68% Test: 90.24%\n",
            "Epoch: 100, Loss: 0.3472, Train: 90.92%, Valid: 89.78% Test: 90.36%\n",
            "Epoch: 101, Loss: 0.3453, Train: 90.98%, Valid: 89.66% Test: 90.38%\n",
            "Epoch: 102, Loss: 0.3458, Train: 90.97%, Valid: 89.66% Test: 90.41%\n",
            "Epoch: 103, Loss: 0.3456, Train: 91.03%, Valid: 89.54% Test: 90.45%\n",
            "Epoch: 104, Loss: 0.3423, Train: 91.06%, Valid: 89.62% Test: 90.37%\n",
            "Epoch: 105, Loss: 0.3423, Train: 91.07%, Valid: 89.62% Test: 90.43%\n",
            "Epoch: 106, Loss: 0.3390, Train: 91.09%, Valid: 89.56% Test: 90.42%\n",
            "Epoch: 107, Loss: 0.3400, Train: 91.10%, Valid: 89.58% Test: 90.41%\n",
            "Epoch: 108, Loss: 0.3400, Train: 91.12%, Valid: 89.74% Test: 90.35%\n",
            "Epoch: 109, Loss: 0.3399, Train: 91.00%, Valid: 89.62% Test: 90.30%\n",
            "Epoch: 110, Loss: 0.3393, Train: 91.20%, Valid: 89.74% Test: 90.45%\n",
            "Epoch: 111, Loss: 0.3376, Train: 91.14%, Valid: 89.60% Test: 90.35%\n",
            "Epoch: 112, Loss: 0.3390, Train: 91.13%, Valid: 89.54% Test: 90.49%\n",
            "Epoch: 113, Loss: 0.3364, Train: 91.24%, Valid: 90.02% Test: 90.57%\n",
            "Epoch: 114, Loss: 0.3365, Train: 91.29%, Valid: 89.98% Test: 90.50%\n",
            "Epoch: 115, Loss: 0.3364, Train: 91.22%, Valid: 89.68% Test: 90.48%\n",
            "Epoch: 116, Loss: 0.3360, Train: 91.23%, Valid: 89.78% Test: 90.54%\n",
            "Epoch: 117, Loss: 0.3337, Train: 91.27%, Valid: 89.86% Test: 90.54%\n",
            "Epoch: 118, Loss: 0.3328, Train: 91.28%, Valid: 89.92% Test: 90.53%\n",
            "Epoch: 119, Loss: 0.3323, Train: 91.37%, Valid: 90.06% Test: 90.48%\n",
            "Epoch: 120, Loss: 0.3323, Train: 91.40%, Valid: 89.96% Test: 90.48%\n",
            "Epoch: 121, Loss: 0.3312, Train: 91.37%, Valid: 89.92% Test: 90.59%\n",
            "Epoch: 122, Loss: 0.3301, Train: 91.34%, Valid: 90.02% Test: 90.52%\n",
            "Epoch: 123, Loss: 0.3291, Train: 91.41%, Valid: 90.02% Test: 90.58%\n",
            "Epoch: 124, Loss: 0.3286, Train: 91.41%, Valid: 89.84% Test: 90.63%\n",
            "Epoch: 125, Loss: 0.3308, Train: 91.42%, Valid: 89.90% Test: 90.59%\n",
            "Epoch: 126, Loss: 0.3295, Train: 91.38%, Valid: 90.02% Test: 90.79%\n",
            "Epoch: 127, Loss: 0.3279, Train: 91.42%, Valid: 90.16% Test: 90.66%\n",
            "Epoch: 128, Loss: 0.3273, Train: 91.45%, Valid: 89.96% Test: 90.57%\n",
            "Epoch: 129, Loss: 0.3254, Train: 91.40%, Valid: 89.70% Test: 90.53%\n",
            "Epoch: 130, Loss: 0.3258, Train: 91.44%, Valid: 89.82% Test: 90.60%\n",
            "Epoch: 131, Loss: 0.3275, Train: 91.44%, Valid: 89.88% Test: 90.53%\n",
            "Epoch: 132, Loss: 0.3231, Train: 91.42%, Valid: 89.96% Test: 90.49%\n",
            "Epoch: 133, Loss: 0.3249, Train: 91.48%, Valid: 90.36% Test: 90.57%\n",
            "Epoch: 134, Loss: 0.3248, Train: 91.54%, Valid: 90.10% Test: 90.64%\n",
            "Epoch: 135, Loss: 0.3224, Train: 91.49%, Valid: 89.88% Test: 90.63%\n",
            "Epoch: 136, Loss: 0.3226, Train: 91.50%, Valid: 89.82% Test: 90.70%\n",
            "Epoch: 137, Loss: 0.3202, Train: 91.48%, Valid: 89.88% Test: 90.70%\n",
            "Epoch: 138, Loss: 0.3193, Train: 91.54%, Valid: 90.06% Test: 90.69%\n",
            "Epoch: 139, Loss: 0.3198, Train: 91.57%, Valid: 90.14% Test: 90.59%\n",
            "Epoch: 140, Loss: 0.3190, Train: 91.56%, Valid: 90.08% Test: 90.60%\n",
            "Epoch: 141, Loss: 0.3202, Train: 91.58%, Valid: 89.96% Test: 90.63%\n",
            "Epoch: 142, Loss: 0.3193, Train: 91.54%, Valid: 90.08% Test: 90.61%\n",
            "Epoch: 143, Loss: 0.3205, Train: 91.57%, Valid: 89.98% Test: 90.65%\n",
            "Epoch: 144, Loss: 0.3179, Train: 91.52%, Valid: 89.86% Test: 90.35%\n",
            "Epoch: 145, Loss: 0.3190, Train: 91.56%, Valid: 90.00% Test: 90.48%\n",
            "Epoch: 146, Loss: 0.3195, Train: 91.66%, Valid: 90.22% Test: 90.62%\n",
            "Epoch: 147, Loss: 0.3183, Train: 91.68%, Valid: 90.14% Test: 90.67%\n",
            "Epoch: 148, Loss: 0.3188, Train: 91.66%, Valid: 90.02% Test: 90.68%\n",
            "Epoch: 149, Loss: 0.3156, Train: 91.61%, Valid: 89.76% Test: 90.62%\n",
            "Epoch: 150, Loss: 0.3149, Train: 91.67%, Valid: 89.94% Test: 90.68%\n",
            "Epoch: 151, Loss: 0.3138, Train: 91.67%, Valid: 90.02% Test: 90.75%\n",
            "Epoch: 152, Loss: 0.3131, Train: 91.71%, Valid: 90.06% Test: 90.66%\n",
            "Epoch: 153, Loss: 0.3128, Train: 91.65%, Valid: 90.04% Test: 90.69%\n",
            "Epoch: 154, Loss: 0.3135, Train: 91.76%, Valid: 89.96% Test: 90.69%\n",
            "Epoch: 155, Loss: 0.3118, Train: 91.85%, Valid: 90.08% Test: 90.76%\n",
            "Epoch: 156, Loss: 0.3106, Train: 91.81%, Valid: 90.26% Test: 90.75%\n",
            "Epoch: 157, Loss: 0.3113, Train: 91.79%, Valid: 90.26% Test: 90.75%\n",
            "Epoch: 158, Loss: 0.3109, Train: 91.78%, Valid: 90.18% Test: 90.71%\n",
            "Epoch: 159, Loss: 0.3100, Train: 91.79%, Valid: 90.10% Test: 90.73%\n",
            "Epoch: 160, Loss: 0.3090, Train: 91.88%, Valid: 90.30% Test: 90.72%\n",
            "Epoch: 161, Loss: 0.3063, Train: 91.86%, Valid: 90.26% Test: 90.69%\n",
            "Epoch: 162, Loss: 0.3068, Train: 91.83%, Valid: 90.14% Test: 90.71%\n",
            "Epoch: 163, Loss: 0.3057, Train: 91.90%, Valid: 90.16% Test: 90.78%\n",
            "Epoch: 164, Loss: 0.3077, Train: 91.88%, Valid: 90.14% Test: 90.74%\n",
            "Epoch: 165, Loss: 0.3074, Train: 91.83%, Valid: 90.12% Test: 90.71%\n",
            "Epoch: 166, Loss: 0.3069, Train: 91.76%, Valid: 90.08% Test: 90.61%\n",
            "Epoch: 167, Loss: 0.3061, Train: 91.85%, Valid: 90.30% Test: 90.73%\n",
            "Epoch: 168, Loss: 0.3069, Train: 91.83%, Valid: 90.38% Test: 90.80%\n",
            "Epoch: 169, Loss: 0.3050, Train: 91.80%, Valid: 90.20% Test: 90.73%\n",
            "Epoch: 170, Loss: 0.3043, Train: 91.87%, Valid: 90.40% Test: 90.75%\n",
            "Epoch: 171, Loss: 0.3054, Train: 91.91%, Valid: 90.38% Test: 90.73%\n",
            "Epoch: 172, Loss: 0.3025, Train: 91.76%, Valid: 90.22% Test: 90.62%\n",
            "Epoch: 173, Loss: 0.3013, Train: 91.85%, Valid: 90.24% Test: 90.67%\n",
            "Epoch: 174, Loss: 0.3015, Train: 91.74%, Valid: 90.00% Test: 90.76%\n",
            "Epoch: 175, Loss: 0.3017, Train: 91.96%, Valid: 90.16% Test: 90.83%\n",
            "Epoch: 176, Loss: 0.3017, Train: 92.04%, Valid: 90.20% Test: 90.80%\n",
            "Epoch: 177, Loss: 0.3017, Train: 92.00%, Valid: 90.02% Test: 90.75%\n",
            "Epoch: 178, Loss: 0.3020, Train: 91.84%, Valid: 89.96% Test: 90.54%\n",
            "Epoch: 179, Loss: 0.3060, Train: 91.77%, Valid: 89.94% Test: 90.54%\n",
            "Epoch: 180, Loss: 0.3045, Train: 92.03%, Valid: 90.28% Test: 90.77%\n",
            "Epoch: 181, Loss: 0.3009, Train: 92.02%, Valid: 90.24% Test: 90.63%\n",
            "Epoch: 182, Loss: 0.3018, Train: 91.91%, Valid: 90.08% Test: 90.61%\n",
            "Epoch: 183, Loss: 0.2988, Train: 92.00%, Valid: 90.28% Test: 90.74%\n",
            "Epoch: 184, Loss: 0.2990, Train: 91.97%, Valid: 89.98% Test: 90.63%\n",
            "Epoch: 185, Loss: 0.3043, Train: 92.08%, Valid: 90.32% Test: 90.86%\n",
            "Epoch: 186, Loss: 0.3007, Train: 92.16%, Valid: 90.56% Test: 90.81%\n",
            "Epoch: 187, Loss: 0.2984, Train: 91.80%, Valid: 89.92% Test: 90.49%\n",
            "Epoch: 188, Loss: 0.3038, Train: 92.16%, Valid: 90.52% Test: 90.73%\n",
            "Epoch: 189, Loss: 0.2959, Train: 92.07%, Valid: 90.28% Test: 90.77%\n",
            "Epoch: 190, Loss: 0.2982, Train: 92.06%, Valid: 90.02% Test: 90.75%\n",
            "Epoch: 191, Loss: 0.2987, Train: 92.05%, Valid: 90.32% Test: 90.69%\n",
            "Epoch: 192, Loss: 0.2958, Train: 92.10%, Valid: 90.48% Test: 90.86%\n",
            "Epoch: 193, Loss: 0.2985, Train: 92.06%, Valid: 90.26% Test: 90.73%\n",
            "Epoch: 194, Loss: 0.2968, Train: 92.14%, Valid: 90.36% Test: 90.73%\n",
            "Epoch: 195, Loss: 0.2969, Train: 92.18%, Valid: 90.32% Test: 90.74%\n",
            "Epoch: 196, Loss: 0.2953, Train: 92.16%, Valid: 90.38% Test: 90.67%\n",
            "Epoch: 197, Loss: 0.2957, Train: 92.15%, Valid: 90.32% Test: 90.84%\n",
            "Epoch: 198, Loss: 0.2960, Train: 91.97%, Valid: 90.34% Test: 90.66%\n",
            "Epoch: 199, Loss: 0.2950, Train: 92.06%, Valid: 90.32% Test: 90.63%\n",
            "Epoch: 200, Loss: 0.2935, Train: 92.03%, Valid: 90.28% Test: 90.52%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_result = test(best_model, sub_graph, split_idx, evaluator, save_model_results=True)\n",
        "train_acc, valid_acc, test_acc = best_result\n",
        "print(f'Best model: '\n",
        "      f'Train: {100 * train_acc:.2f}%, '\n",
        "      f'Valid: {100 * valid_acc:.2f}% '\n",
        "      f'Test: {100 * test_acc:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KcoDqzpUnFl",
        "outputId": "9a69f214-f95a-4e7d-ef26-89cab95ef57a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Model Predictions\n",
            "Best model: Train: 92.16%, Valid: 90.56% Test: 90.81%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-bd2bcc944472>:53: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  x = self.softmax(x)\n"
          ]
        }
      ]
    }
  ]
}